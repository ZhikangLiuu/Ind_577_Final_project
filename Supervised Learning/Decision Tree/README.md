# Lecture 8.1 Decision and Regression Trees

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/RandyRDavila/Data_Science_and_Machine_Learning_Spring_2022/blob/main/Lecture_8/Lecture_8_1.ipynb)

**Decision Trees (DTs)** are a non-parametric supervised learning method used for classification and regression. The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features. A tree can be seen as a piecewise constant approximation.

Decision trees tend to be the method of choice for predictive modeling because they are relatively easy to understand and are also very effective. The basic goal of a decision tree is to split a population of data into smaller segments. There are two stages to prediction. The first stage is training the model—this is where the tree is built, tested, and optimized by using an existing collection of data. In the second stage, you actually use the model to predict an unknown outcome.

Decision trees are constructed from only two elements — nodes and branches.

<img src="Decision_Tree_Example1.jpeg" alt="Drawing" style="width: 500px;"/>

This image was taken directly from this [article.](https://betterdatascience.com/mml-decision-trees/)


The nodes shown above fall under the following types of nodes:

* Root node — node at the top of the tree. This node acts as the input node for feature vectors in the model. 
* Decision nodes — nodes where the variables are evaluated. These nodes have arrows pointing to them and away from them
* Leaf nodes — final nodes at which the prediction is made

To illustrate how decision trees work we will consider artificial binary classification data generated by the ```sklearn.datasets.make_moons()``` function. One instance of this data is generated by running the following code cell. 


![image](https://github.com/ZhikangLiuu/Ind_577_Final_project/assets/165843914/081f3ed8-c3d2-4fee-9cf9-fff604f1fd18)



The figure above depicts a graph theoretic tree that is used to make predictions. Suppose you would like to make a prediction on a given feature vector $x = [x_0, x_1]^T$. To do this, follow these steps: 

1. Start at the *root node* (depth 0, at the top).
2. If $x_1 \le 0.268$, then you then move down to the root's left child node (depth 1, left), otherwise move down to the root's right child node (depth 1, right). 
3. Repeat the process (illustrated in 2) of moving to successive child nodes according to satifying the boolean condition specified at each parent node until you reach a leaf node (a node with no child nodes). 
4. The predicted class of this leaf node will be the predicted class of our feature vector $x = [x_0, x_1]^T$.

That's it! We can visualize the decision regions generated by our trained decision tree by running the following code cell. 
